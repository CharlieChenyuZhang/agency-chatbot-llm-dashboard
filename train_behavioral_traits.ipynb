{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Behavioral Trait Probes\n",
        "\n",
        "This notebook trains probes to detect behavioral traits: Rigidity, Independence, and Goal Persistence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('src/')\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from losses import edl_mse_loss\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from behavioral_dataset import BehavioralTraitDataset, create_behavioral_dataset\n",
        "from behavioral_traits_config import (\n",
        "    BEHAVIORAL_TRAIT_LABELS, \n",
        "    BEHAVIORAL_DATASET_DIRS,\n",
        "    BEHAVIORAL_TRAINING_CONFIG\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "from probes import ProbeClassification, ProbeClassificationMixScaler, LinearProbeClassification\n",
        "from train_test_utils import train, test \n",
        "import torch.nn as nn\n",
        "\n",
        "import time\n",
        "import pickle\n",
        "import sklearn.model_selection\n",
        "import numpy as np\n",
        "\n",
        "tic, toc = (time.time, time.time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "with open('hf_access_token.txt', 'r') as file:\n",
        "    access_token = file.read().strip()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-4-Scout-17B-16E-Instruct\", token=access_token, padding_side='left')\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-4-Scout-17B-16E-Instruct\", token=access_token)\n",
        "model.half().cuda()\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrainerConfig:\n",
        "    # optimization parameters\n",
        "    learning_rate = 1e-3\n",
        "    betas = (0.9, 0.95)\n",
        "    weight_decay = 0.1 # only applied on matmul weights\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k,v in kwargs.items():\n",
        "            setattr(self, k, v)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "new_prompt_format = True\n",
        "residual_stream = True\n",
        "uncertainty = False\n",
        "logistic = True\n",
        "augmented = False\n",
        "remove_last_ai_response = True\n",
        "include_inst = True\n",
        "one_hot = False  # Set to False for behavioral traits\n",
        "regression_mode = False  # Set to True for continuous prediction\n",
        "\n",
        "# Behavioral traits to train\n",
        "behavioral_traits = [\"rigidity\", \"independence\", \"goal_persistence\"]\n",
        "\n",
        "accuracy_dict = {}\n",
        "torch_device = \"cuda\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop for Behavioral Traits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for trait_type in behavioral_traits:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {trait_type.upper()} probe\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Get directories for this trait\n",
        "    directories = BEHAVIORAL_DATASET_DIRS[trait_type]\n",
        "    \n",
        "    # Create dataset\n",
        "    dataset = create_behavioral_dataset(\n",
        "        trait_type=trait_type,\n",
        "        directory=directories[0],  # Use first directory as primary\n",
        "        tokenizer=tokenizer,\n",
        "        model=model,\n",
        "        convert_to_llama2_format=True,\n",
        "        additional_datas=directories[1:] if len(directories) > 1 else None,\n",
        "        new_format=new_prompt_format,\n",
        "        residual_stream=residual_stream,\n",
        "        if_augmented=augmented,\n",
        "        remove_last_ai_response=remove_last_ai_response,\n",
        "        include_inst=include_inst,\n",
        "        k=1,\n",
        "        one_hot=one_hot,\n",
        "        regression_mode=regression_mode\n",
        "    )\n",
        "    \n",
        "    print(f\"Dataset size: {len(dataset)}\")\n",
        "    print(f\"Label distribution: {dict(zip(*np.unique(dataset.labels, return_counts=True)))}\")\n",
        "    \n",
        "    # Train-test split\n",
        "    train_size = int(BEHAVIORAL_TRAINING_CONFIG['train_split'] * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_idx, val_idx = sklearn.model_selection.train_test_split(\n",
        "        list(range(len(dataset))), \n",
        "        test_size=test_size,\n",
        "        train_size=train_size,\n",
        "        random_state=BEHAVIORAL_TRAINING_CONFIG['random_state'],\n",
        "        shuffle=True,\n",
        "        stratify=dataset.labels if not regression_mode else None\n",
        "    )\n",
        "\n",
        "    train_dataset = Subset(dataset, train_idx)\n",
        "    test_dataset = Subset(dataset, val_idx)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, \n",
        "        shuffle=True, \n",
        "        pin_memory=True, \n",
        "        batch_size=BEHAVIORAL_TRAINING_CONFIG['batch_size'], \n",
        "        num_workers=1\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, \n",
        "        shuffle=False, \n",
        "        pin_memory=True, \n",
        "        batch_size=BEHAVIORAL_TRAINING_CONFIG['test_batch_size'], \n",
        "        num_workers=1\n",
        "    )\n",
        "\n",
        "    # Loss function\n",
        "    if uncertainty:\n",
        "        loss_func = edl_mse_loss\n",
        "    elif regression_mode:\n",
        "        loss_func = nn.MSELoss()  # Use MSE for regression\n",
        "    else:\n",
        "        loss_func = nn.CrossEntropyLoss()  # Use CrossEntropy for classification\n",
        "\n",
        "    # Initialize accuracy tracking\n",
        "    accuracy_dict[trait_type] = []\n",
        "    accuracy_dict[trait_type + \"_final\"] = []\n",
        "    accuracy_dict[trait_type + \"_train\"] = []\n",
        "    \n",
        "    accs = []\n",
        "    final_accs = []\n",
        "    train_accs = []\n",
        "    \n",
        "    # Train probes for each layer\n",
        "    for i in tqdm(range(0, 41), desc=f\"Training {trait_type} probes\"):\n",
        "        trainer_config = TrainerConfig()\n",
        "        \n",
        "        # Create probe\n",
        "        num_classes = len(BEHAVIORAL_TRAIT_LABELS[trait_type]) if not regression_mode else 1\n",
        "        probe = LinearProbeClassification(\n",
        "            probe_class=num_classes, \n",
        "            device=\"cuda\", \n",
        "            input_dim=5120,\n",
        "            logistic=logistic\n",
        "        )\n",
        "        \n",
        "        optimizer, scheduler = probe.configure_optimizers(trainer_config)\n",
        "        best_acc = 0\n",
        "        max_epoch = BEHAVIORAL_TRAINING_CONFIG['max_epochs']\n",
        "        verbosity = False\n",
        "        layer_num = i\n",
        "        \n",
        "        print(f\"\\n{'-' * 40} Layer {layer_num} {'-' * 40}\")\n",
        "        \n",
        "        for epoch in range(1, max_epoch + 1):\n",
        "            if epoch == max_epoch:\n",
        "                verbosity = True\n",
        "            \n",
        "            # Training\n",
        "            if uncertainty:\n",
        "                train_results = train(\n",
        "                    probe, torch_device, train_loader, optimizer, \n",
        "                    epoch, loss_func=loss_func, verbose_interval=None,\n",
        "                    verbose=verbosity, layer_num=layer_num, \n",
        "                    return_raw_outputs=True, epoch_num=epoch, \n",
        "                    num_classes=num_classes\n",
        "                )\n",
        "                test_results = test(\n",
        "                    probe, torch_device, test_loader, loss_func=loss_func, \n",
        "                    return_raw_outputs=True, verbose=verbosity, layer_num=layer_num,\n",
        "                    scheduler=scheduler, epoch_num=epoch, \n",
        "                    num_classes=num_classes\n",
        "                )\n",
        "            else:\n",
        "                train_results = train(\n",
        "                    probe, torch_device, train_loader, optimizer, \n",
        "                    epoch, loss_func=loss_func, verbose_interval=None,\n",
        "                    verbose=verbosity, layer_num=layer_num,\n",
        "                    return_raw_outputs=True,\n",
        "                    one_hot=one_hot, num_classes=num_classes\n",
        "                )\n",
        "                test_results = test(\n",
        "                    probe, torch_device, test_loader, loss_func=loss_func, \n",
        "                    return_raw_outputs=True, verbose=verbosity, layer_num=layer_num,\n",
        "                    scheduler=scheduler,\n",
        "                    one_hot=one_hot, num_classes=num_classes\n",
        "                )\n",
        "\n",
        "            # Save best model\n",
        "            if test_results[1] > best_acc:\n",
        "                best_acc = test_results[1]\n",
        "                torch.save(\n",
        "                    probe.state_dict(), \n",
        "                    f\"probe_checkpoints/behavioral_probes/{trait_type}_probe_at_layer_{layer_num}.pth\"\n",
        "                )\n",
        "        \n",
        "        # Save final model\n",
        "        torch.save(\n",
        "            probe.state_dict(), \n",
        "            f\"probe_checkpoints/behavioral_probes/{trait_type}_probe_at_layer_{layer_num}_final.pth\"\n",
        "        )\n",
        "        \n",
        "        accs.append(best_acc)\n",
        "        final_accs.append(test_results[1])\n",
        "        train_accs.append(train_results[1])\n",
        "        \n",
        "        # Plot confusion matrix\n",
        "        if not regression_mode:\n",
        "            cm = confusion_matrix(test_results[3], test_results[2])\n",
        "            cm_display = ConfusionMatrixDisplay(\n",
        "                cm, \n",
        "                display_labels=list(BEHAVIORAL_TRAIT_LABELS[trait_type].keys())\n",
        "            ).plot()\n",
        "            plt.title(f\"{trait_type.capitalize()} - Layer {layer_num}\")\n",
        "            plt.show()\n",
        "\n",
        "        # Update accuracy dict\n",
        "        accuracy_dict[trait_type].append(accs)\n",
        "        accuracy_dict[trait_type + \"_final\"].append(final_accs)\n",
        "        accuracy_dict[trait_type + \"_train\"].append(train_accs)\n",
        "        \n",
        "        # Save intermediate results\n",
        "        with open(\"probe_checkpoints/behavioral_probes_experiment.pkl\", \"wb\") as outfile:\n",
        "            pickle.dump(accuracy_dict, outfile)\n",
        "    \n",
        "    # Clean up\n",
        "    del dataset, train_dataset, test_dataset, train_loader, test_loader\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nTraining completed for all behavioral traits!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot results for each trait\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "for i, trait_type in enumerate(behavioral_traits):\n",
        "    if trait_type in accuracy_dict:\n",
        "        accs = accuracy_dict[trait_type][-1]  # Get the last (complete) results\n",
        "        axes[i].plot(range(len(accs)), accs, 'b-', label='Best Accuracy')\n",
        "        axes[i].set_title(f'{trait_type.capitalize()} Probe Accuracy')\n",
        "        axes[i].set_xlabel('Layer')\n",
        "        axes[i].set_ylabel('Accuracy')\n",
        "        axes[i].grid(True)\n",
        "        axes[i].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print best results for each trait\n",
        "print(\"\\nBest Results:\")\n",
        "for trait_type in behavioral_traits:\n",
        "    if trait_type in accuracy_dict:\n",
        "        accs = accuracy_dict[trait_type][-1]\n",
        "        best_layer = np.argmax(accs)\n",
        "        best_acc = max(accs)\n",
        "        print(f\"{trait_type.capitalize()}: {best_acc:.3f} at layer {best_layer}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
